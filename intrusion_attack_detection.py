# -*- coding: utf-8 -*-
"""Intrusion Attack Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bPEAW8dFpNBtFGEJl3NFYXCfRNskpV65

# Intrusion Attack Detection Project Framework - Data Science

Upload files
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np

"""Read datasheet with the help of pandas"""

datasheet = pd.read_csv("data9_1.csv")
datasheet.head()

"""info() -> infromation about the data"""

datasheet.info(5)

"""Selected only those columns which are required to make our desired model. So we copy that columns from that main datasheet and decleared into 3 different variable as (rds_phase1, rds_phase2, rds_phase3)

Step:1 Select required columns for phase 1

Here, declears that all rows with specified columns data selected by using -> .loc[starting_row : ending_row, ["Selected comuns"........]
"""

##required datasheet rds for phase_1 or R-phase
# rds_phase1 = datasheet.loc[:,['R1-PA1:VH','R1-PA4:IH','R1-PM4:I','R1-PM1:V']]
# rds_phase1.head()

"""Here, declears that only one row(0) with specified columns data selected by using -> .loc[starting_row : ending_row, ["Selected comuns"........]"""

# required datasheet rds for phase_1 or R-phase
rds_phase1 = datasheet.loc[0:0,['R1-PA1:VH','R1-PA4:IH','R1-PM4:I','R1-PM1:V']]
rds_phase1

"""Creat a loop so that all rows data are fitted to in array as a same order"""



"""Find out the size of array and then matched with the desired model array"""

# see the error message to find the array of size
# numpy.reshape(array, shape, order = 'C')
# shape array with 4 rows and 2 columns

# rds_phase1.to_numpy().reshape(2,2,2)

"""From above,we know that the size of this array 62328
and now we reshape it as a (2x2) 3D-matrix
"""

# we have found array of size: 62328
# now to convert dataframe to 3D
# we have to divide 62328 in 3 parts such that multiplication of these 3 parts give us 42168
# arr_phase1 = rds_phase1.to_numpy().reshape(15582, 2, 2)
# arr_phase1

rds_phase1.to_numpy().reshape(2,2,2)

"""From above,we know that the size of this array 4
and now we reshape it as a (1x2x2) 3D-matrix
"""

arr_phase1 = rds_phase1.to_numpy().reshape(1, 2, 2)
arr_phase1

""".ndim -> used to find out array dimension"""

# display dimensions
# 3 means the dataframe is 3D
arr_phase1.ndim

"""Step:1 Select required columns for phase 2 and       //* follow same steps as previously done"""

# # required datasheet rds for phase_2 or Y-phase
# rds_phase2 = datasheet.loc[:,['R1-PA2:VH','R1-PA5:IH','R1-PM5:I','R1-PM2:V']]
# rds_phase2.head()

rds_phase2 = datasheet.loc[0:0,['R1-PA2:VH','R1-PM5:I','R1-PM2:V','R1-PA5:IH']]
rds_phase2.head()

# arr_phase2 = rds_phase2.to_numpy().reshape(15582, 2, 2)
# arr_phase2

arr_phase2 = rds_phase2.to_numpy().reshape(1, 2, 2)
arr_phase2

arr_phase2.ndim

"""Step:1 Select required columns for phase 3 and       //* follow same steps as previously done"""

# required datasheet rds for phase_3 or B-phase
# rds_phase3 = datasheet.loc[:,['R1-PA3:VH','R1-PA6:IH','R1-PM6:I','R1-PM3:V']]
# rds_phase3

rds_phase3 = datasheet.loc[0:0,['R1-PA3:VH','R1-PM6:I','R1-PM3:V','R1-PA6:IH']]
rds_phase3

# arr_phase3 = rds_phase3.to_numpy().reshape(15582, 2, 2)
# arr_phase3

arr_phase3 = rds_phase3.to_numpy().reshape(1, 2, 2)
arr_phase3

arr_phase3.ndim

"""Concatenation of array"""

arr_phase1

arr_phase2

arr_phase3

# Concatenation method (axis= 1 for same column) and (axis= -1 for same row)
print(np.concatenate((arr_phase1, arr_phase2, arr_phase3), axis = 1))

"""Create a file _Demo Purpose"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import warnings

import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import sklearn.metrics as metrics

from sklearn.neighbors import KNeighborsClassifier as KNN
import numpy as np

# Load dataset
from sklearn.datasets import load_iris
iris = load_iris()

X = iris.data
y = iris.target

# Split dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2018)

# import KNeighborsClassifier model
knn = KNN(n_neighbors=3)

# train model
knn.fit(X_train, y_train)

from joblib import Parallel, delayed
import joblib


# Save the model as a pickle in a file
joblib.dump(knn, 'filename.pkl')

# Load the model from the file
knn_from_joblib = joblib.load('filename.pkl')

# Use the loaded model to make predictions
knn_from_joblib.predict(X_test)

